{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline\n",
    "Initial data analysis pipeline including a naive sentiment analysis using TextBlob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calmap # for making GitHub-style calendar plots of time-series\n",
    "# Plot using Pandas datatime objects\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "rc_fonts = {'axes.labelsize': 20,\n",
    "            'xtick.labelsize': 16,\n",
    "            'ytick.labelsize': 16}\n",
    "plt.rcParams.update(rc_fonts)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn for TF-IDF and similarity detection\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import MDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use ```spaCy``` for tokenization and sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "# Load spaCy language model (blank model to which we add pipeline components)\n",
    "sentencizer = spacy.blank('en')\n",
    "sentencizer.add_pipe(sentencizer.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use ```flair``` NLP library for sentiment prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify named entity of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Ryan Lochte\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write data: Boolean\n",
    "Specify if we want to write the output data to csv or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_ = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>publication</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agent Cooper in Twin Peaks is the audience: on...</td>\n",
       "      <td>Tasha Robinson</td>\n",
       "      <td>2017-05-31</td>\n",
       "      <td>And never more so than in Showtime’s new...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>2376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI, the humanity!</td>\n",
       "      <td>Sam Byford</td>\n",
       "      <td>2017-05-30</td>\n",
       "      <td>AlphaGo’s victory isn’t a defeat for hum...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>2125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Viral Machine</td>\n",
       "      <td>Kaitlyn Tiffany</td>\n",
       "      <td>2017-05-25</td>\n",
       "      <td>Super Deluxe built a weird internet empi...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>3310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How Anker is beating Apple and Samsung at thei...</td>\n",
       "      <td>Nick Statt</td>\n",
       "      <td>2017-05-22</td>\n",
       "      <td>Steven Yang quit his job at Google in th...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>3632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tour Black Panther’s reimagined homeland with ...</td>\n",
       "      <td>Kwame Opam</td>\n",
       "      <td>2017-05-15</td>\n",
       "      <td>Ahead of Black Panther’s 2018 theatrical...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title           author  \\\n",
       "0  Agent Cooper in Twin Peaks is the audience: on...   Tasha Robinson   \n",
       "1                                  AI, the humanity!       Sam Byford   \n",
       "2                                  The Viral Machine  Kaitlyn Tiffany   \n",
       "3  How Anker is beating Apple and Samsung at thei...       Nick Statt   \n",
       "4  Tour Black Panther’s reimagined homeland with ...       Kwame Opam   \n",
       "\n",
       "        date                                            content    year  \\\n",
       "0 2017-05-31        And never more so than in Showtime’s new...  2017.0   \n",
       "1 2017-05-30        AlphaGo’s victory isn’t a defeat for hum...  2017.0   \n",
       "2 2017-05-25        Super Deluxe built a weird internet empi...  2017.0   \n",
       "3 2017-05-22        Steven Yang quit his job at Google in th...  2017.0   \n",
       "4 2017-05-15        Ahead of Black Panther’s 2018 theatrical...  2017.0   \n",
       "\n",
       "   month publication  length  \n",
       "0    5.0       Verge    2376  \n",
       "1    5.0       Verge    2125  \n",
       "2    5.0       Verge    3310  \n",
       "3    5.0       Verge    3632  \n",
       "4    5.0       Verge     262  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafile = 'all_the_news_v2.csv'\n",
    "datapath = Path('../') / 'data' / datafile \n",
    "colnames = ['title', 'author', 'date', 'content', 'year', 'month', 'publication', 'length']\n",
    "\n",
    "news = pd.read_csv(datapath, usecols=colnames, parse_dates=['date'])\n",
    "news['author'] = news['author'].str.strip()\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143156"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = news.dropna(subset=['date', 'title'])\n",
    "news.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                  143156\n",
       "unique                   1480\n",
       "top       2017-01-13 00:00:00\n",
       "freq                      415\n",
       "first     2000-05-15 00:00:00\n",
       "last      2017-07-05 00:00:00\n",
       "Name: date, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news['date'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter articles based on name match\n",
    "In this section we only select those news articles that contain part of or all of the name we input as ```name```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>publication</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3095</th>\n",
       "      <td>After Olympic-sized goof, Ryan Lochte begins a...</td>\n",
       "      <td>David Wharton</td>\n",
       "      <td>2017-04-27</td>\n",
       "      <td>Don’t cringe or shake your head or stop readin...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Los Angeles Times</td>\n",
       "      <td>1925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8946</th>\n",
       "      <td>Michael Phelps is a touch off in 100 butterfly...</td>\n",
       "      <td>Everett Cook</td>\n",
       "      <td>2014-08-08</td>\n",
       "      <td>Michael Phelps\\' bid for his most important wi...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Los Angeles Times</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13789</th>\n",
       "      <td>Michael Phelps Powers U.S. to Victory in 4x100...</td>\n",
       "      <td>Victor Mather, Karen Crouse and Doug Mills</td>\n",
       "      <td>2016-08-10</td>\n",
       "      <td>RIO DE JANEIRO — Michael Phelps won his 19th O...</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13825</th>\n",
       "      <td>Rio Olympics: Simone Manuel Makes History in t...</td>\n",
       "      <td>Karen Crouse</td>\n",
       "      <td>2016-08-16</td>\n",
       "      <td>RIO DE JANEIRO — Simone Manuel managed to make...</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>1468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13839</th>\n",
       "      <td>Rio Olympics: A Phelps Upset, a Judo Snub, and...</td>\n",
       "      <td>Sam Manchester and Victor Mather</td>\n",
       "      <td>2016-08-16</td>\n",
       "      <td>Katie Ledecky did what Katie Ledecky does best...</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>1614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "3095   After Olympic-sized goof, Ryan Lochte begins a...   \n",
       "8946   Michael Phelps is a touch off in 100 butterfly...   \n",
       "13789  Michael Phelps Powers U.S. to Victory in 4x100...   \n",
       "13825  Rio Olympics: Simone Manuel Makes History in t...   \n",
       "13839  Rio Olympics: A Phelps Upset, a Judo Snub, and...   \n",
       "\n",
       "                                           author       date  \\\n",
       "3095                                David Wharton 2017-04-27   \n",
       "8946                                 Everett Cook 2014-08-08   \n",
       "13789  Victor Mather, Karen Crouse and Doug Mills 2016-08-10   \n",
       "13825                                Karen Crouse 2016-08-16   \n",
       "13839            Sam Manchester and Victor Mather 2016-08-16   \n",
       "\n",
       "                                                 content    year  month  \\\n",
       "3095   Don’t cringe or shake your head or stop readin...  2017.0    4.0   \n",
       "8946   Michael Phelps\\' bid for his most important wi...  2014.0    8.0   \n",
       "13789  RIO DE JANEIRO — Michael Phelps won his 19th O...  2016.0    8.0   \n",
       "13825  RIO DE JANEIRO — Simone Manuel managed to make...  2016.0    8.0   \n",
       "13839  Katie Ledecky did what Katie Ledecky does best...  2016.0    8.0   \n",
       "\n",
       "             publication  length  \n",
       "3095   Los Angeles Times    1925  \n",
       "8946   Los Angeles Times     996  \n",
       "13789     New York Times     276  \n",
       "13825     New York Times    1468  \n",
       "13839     New York Times    1614  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_name(content, name):\n",
    "    flag = False\n",
    "    if name in content:\n",
    "        flag = True\n",
    "    return flag\n",
    "\n",
    "def filter_df(df):\n",
    "    df['match'] = df['content'].apply(lambda x: check_name(x, name))\n",
    "    df_relevant = df.loc[df['match'].eq(True)]\n",
    "    return df_relevant.drop(['match'], axis=1)\n",
    "\n",
    "news_relevant = filter_df(news)\n",
    "print(news_relevant.shape[0])\n",
    "news_relevant.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform sentence segmentation\n",
    "Store the sentences in each news articles as a list of sentences, from which we can easily extract per-sentence sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant(text, name):\n",
    "    doc = sentencizer(text)\n",
    "    relevant = []\n",
    "    for sent in doc.sents:\n",
    "        for n in name.split():\n",
    "            if n in sent.text:\n",
    "                clean = sent.text.replace(\"\\n\", \" \").replace(\"\\xa0\", \" \")\n",
    "                # Strip bad characters at the start of sentences\n",
    "                clean = clean.strip(\"[\\'\").strip(\"\\']\").strip('\\\"').strip(\"\\'\\\"\")\n",
    "                clean = clean.strip(\",\\'\").strip(\"\\',\").strip('\\\"').strip(\"\\'\\\"\").strip()\n",
    "                relevant.append(clean)\n",
    "    # Remove duplicates\n",
    "    relevant = list(dict.fromkeys(relevant))\n",
    "    return relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_relevant['relevant'] = news_relevant['content'].apply(lambda x: get_relevant(x, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"That’s all Ryan Lochte wants as he ambles across the pool deck on a bright Southern California day, looking tanned and relaxed, if a bit weary from his morning workout.', '“\", 'Lochte now finds himself living in Los Angeles with his pregnant fiancée and training at USC with thoughts of a comeback.', \"Here’s the condensed version:', 'After a fifth-place finish in the 200-meter individual medley — his dissatisfaction hardly assuaged by a gold in the 800 freestyle relay — Lochte partied with three teammates at the French team’s hospitality house.',\", \"Lochte initially told authorities they were pulled over and robbed by armed men posing as police officers.',\", \"Lochte suffered an additional blow as Speedo and other corporate sponsors walked away.',\", 'Ryan Lochte didn’t have to get drunk and vandalize a Rio de Janeiro gas station during the Summer Olympics in August.', \"But he did, and it’s at times like this when people really need their...', 'Ryan Lochte didn’t have to get drunk and vandalize a Rio de Janeiro gas station during the Summer Olympics in August.\", \"Lochte offers a different perspective, contending that Rio was the culmination of a dark period in his life.',\", \"If you are looking for a single motivation, one moment that turned Lochte around, it isn’t that simple.',\", \"If anything, the incident might have made Lochte a more sympathetic figure in public.',\", \"A day after news broke that Ryan Lochte will be suspended from swimming for 10 months, the three teammates who accompanied him on a night of partying at the Summer Olympics have received their punishment for fabricating stories of a robbery.',\", \"Gunnar Bentz, Jack Conger and James Feigen were handed...', 'A day after news broke that Ryan Lochte will be suspended from swimming for 10 months, the three teammates who accompanied him on a night of partying at the Summer Olympics have received their punishment for fabricating stories of a robbery.',\", 'Like a lot of athletes, Lochte had never been able to separate competition and personal life.', 'If Lochte needed an extra push toward the pool, it might have come from a visit to the doctor with his fiancée.', \"The boy will be about 3 when the 2020 Tokyo Olympics come around, Lochte figured.', '“\", \"Ryan Lochte says he feels “a little hurt” after being involved in an altercation on “Dancing with the Stars” that prompted producers to cut to commercials.\\\\xa0', 'The beleaguered swimmer was reportedly rushed by an unknown pair of men wearing anti-Lochte T-shirts while receiving his scores from judge...', 'Ryan Lochte says he feels “a little hurt” after being involved in an altercation on “Dancing with the Stars” that prompted producers to cut to commercials.\\\\xa0', 'The beleaguered swimmer was reportedly rushed by an unknown pair of men wearing anti-Lochte T-shirts while receiving his scores from judge...', '“I needed something to knock me down to the lowest,” Lochte says. “\", 'The younger guys tease him about having “old-man strength,” but Lochte cannot train as hard as he once did.', 'Scheduled to compete in several events beginning Friday, Lochte feels jittery about racing for the first time since the Olympics.', 'The thing is, Lochte doesn’t entirely fit his renegade image.'] \n",
      "--\n",
      "['Ryan Lochte, another preeminent American swimmer, finished fifth in 52.21.', 'But he has never competed in an Olympics, which could be why, during a television interview after the race, Shields said, \"I was so stoked to be racing in between Ryan and Michael.'] \n",
      "--\n",
      "['Ryan Held and Nathan Adrian brought the U.S. home Adrian’s split was an even better 46.97.', 'France and the United States split the last two Olympic gold medals, with Yannick Agnel of France outdueling Ryan Lochte in the anchor leg to win four years ago.'] \n",
      "--\n",
      "['Michael Phelps won the 200 individual medley easily, beating not only teammate Ryan Lochte, but an ancient Greek.', 'Lochte finished fifth.'] \n",
      "--\n",
      "['Phelps Breaks an Ancient Record: Michael Phelps won the 200 individual medley easily, beating not only teammate Ryan Lochte, but an ancient Greek.', 'Lochte finished fifth.'] \n",
      "--\n"
     ]
    }
   ],
   "source": [
    "for i in news_relevant['relevant'][:5]:\n",
    "    print(i, '\\n--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment scoring using ```flair```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to trained flair PyTorch model\n",
    "model_path = '../modules/classification/flair/models/elmo_md/final-model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-07 06:36:31,076 loading file ../modules/classification/flair/models/elmo_md/final-model.pt\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "classifier = TextClassifier.load_from_file(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess text and tokenize as per FastText requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews = [\n",
    "#     \"This restaurant literally changed my life. This is the best food I've ever eaten!\",\n",
    "#     \"I do not like this place at all. They were very rude.\",\n",
    "#     \"I don't know. It was ok, I guess. Not really sure what to say.\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_flair(text_list):\n",
    "    scores = []\n",
    "    for item in text_list:\n",
    "        sentence = Sentence(item)\n",
    "        classifier.predict(sentence)\n",
    "        scores.append(int(sentence.labels[0].value))\n",
    "    sentiment_list = [(s-3)/2 if s else 0 for s in scores]\n",
    "    score = np.mean(sentiment_list)\n",
    "    deviation = np.std(sentiment_list)\n",
    "    return score, deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_relevant['score'], news_relevant['deviation'] = zip(*news_relevant['relevant'].map(get_score_flair))\n",
    "news_relevant.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize relevant sentences for comparison\n",
    "This is to remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_removed_words = {n.lower() for n in name.split()}\n",
    "# Include specific words to be removed\n",
    "stopwords = sentencizer.Defaults.stop_words\n",
    "stopwords = stopwords.union(add_removed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and lemmatize text\n",
    "def lemmatize(text):\n",
    "    doc = sentencizer(text)\n",
    "    tokens = [str(tok.lemma_).lower() for tok in doc if tok.text not in stopwords \\\n",
    "              and tok.text not in punctuation]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_relevant['lemmas'] = news_relevant['relevant'].str.join(' ').apply(lemmatize).str.join(' ')\n",
    "news_relevant[['relevant', 'lemmas']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_relevant = news_relevant.drop_duplicates(subset=['lemmas'])\n",
    "news_relevant.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive sentiment group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = news_relevant[news_relevant['score'] > 0.0].sort_values(by=['score'], ascending=False).reset_index(drop=True)\n",
    "print(\"Found {} overall positive articles for {}\".format(pos.shape[0], name))\n",
    "pos.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write positive results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_:\n",
    "    out_filename = '_'.join(name.split()).lower() + '_pos.csv'\n",
    "    out_path = Path('./') / \"results/flair\" / out_filename\n",
    "    pos.sort_values(by='publication')[['publication', 'title', 'date', 'relevant', 'score', 'deviation']] \\\n",
    "                    .to_csv(out_path, index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative sentiment group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = news_relevant[news_relevant['score'] < 0.0].sort_values(by=['score']).reset_index(drop=True)\n",
    "print(\"Found {} overall negative articles for {}\".format(neg.shape[0], name))\n",
    "neg.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write negative results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_:\n",
    "    out_filename = '_'.join(name.split()).lower() + '_neg.csv'\n",
    "    out_path = Path('./') / \"results/flair\" / out_filename\n",
    "    neg.sort_values(by='publication')[['publication', 'title', 'date', 'relevant', 'score', 'deviation']] \\\n",
    "                    .to_csv(out_path, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed = news_relevant[news_relevant['score'] == 0.0].reset_index(drop=True)\n",
    "print(\"Found {} overall mixed articles for {}\".format(mixed.shape[0], name))\n",
    "mixed.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highlight relevant named entities using ```spaCy```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "options = {'ents': ['PERSON', 'ORG', 'GPE', 'EVENT'], \n",
    "           'colors': {'PERSON': '#9fafe5', 'ORG': '#d59b9b', 'GPE':'#81cba6'}}\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "def display_entities(nlp, df, max_entries=5):\n",
    "    # Set relevant named entities that we want to extract\n",
    "    for idx, sent in enumerate(df['relevant'].str.join(' ')[:max_entries]):\n",
    "        doc = nlp(sent)\n",
    "        printmd('**{}**'.format(df['title'][idx]))\n",
    "        displacy.render(doc, style='ent', jupyter=True, options=options)\n",
    "        print('\\n')\n",
    "        \n",
    "def vis(pos, neg, mixed, spacy_lang='en_core_web_md'):\n",
    "    nlp = spacy.load(spacy_lang)\n",
    "    # Visualize positive and negativ groups using markdown\n",
    "    printmd('<font color=green>**Positive**</font>')\n",
    "    display_entities(nlp, pos)\n",
    "    printmd('<font color=red>**Negative**</font>')\n",
    "    display_entities(nlp, neg)\n",
    "    printmd('<font color=yellow>**Mixed**</font>')\n",
    "    display_entities(nlp, mixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis(pos, neg, mixed, spacy_lang='en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "### Plot sentiment score and magnitude versus time of publishing of the article\n",
    "In this section, sentiment \"score\" is the median of all polarity values (positive or negative) obtained per-sentence of the article from TextBlob. Sentiment \"magnitude\" is the standard deviation of sentiment among the per-sentence polarity values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_avg_score = news_relevant.groupby('date')['score'].mean()\n",
    "news_avg_dev = news_relevant.groupby('date')['deviation'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get article count per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_count = news_relevant.groupby(['date']).count()['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get peak polar article per day (min negative or max positive score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_relevant['abs'] = news_relevant['score'].abs()\n",
    "news_relevant[['date', 'score', 'abs']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_peak_polar = news_relevant.groupby('date').max()[['title', 'publication', 'relevant']]\n",
    "# Extract just the first 3 relevant sentences from the article and convert to single string\n",
    "news_peak_polar['relevant'] = news_peak_polar['relevant'].apply(lambda x: x[:3]).str.join(' ')\n",
    "print(news_peak_polar.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine scores, magnitudes and article counts per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.concat((news_avg_score, news_avg_dev, news_count), axis=1).sort_values(by=['date'])\n",
    "scores.columns = ['mean_score', 'mean_dev', 'count']\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate scores/counts DataFrame with most polar news content for that day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat((news_peak_polar, scores), axis=1).sort_index()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reindex data to show daily scores\n",
    "Since we have really sparse data (news articles about the target are not written every day, we reindex the time series and fill missing values with zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.date_range('1/1/2014', '7/5/2017')\n",
    "daily = data.reindex(idx, fill_value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8))\n",
    "ax1.fill_between(daily.index, daily['mean_score'], step='mid', color='black', alpha=0.6, linewidth=4);\n",
    "ax1.set_ylabel('Mean Score');\n",
    "ax1.set_title('Sentiment scores and deviations with time for \"{}\"'.format(name), size=15);\n",
    "ax2.fill_between(daily.index, daily['mean_dev'], step='mid', color='black', alpha=0.6, linewidth=4);\n",
    "ax2.set_ylabel('Mean Deviation');\n",
    "ax2.set_xlabel('Date');\n",
    "# Initiate a second y-axis with a shared x-axis for the article counts\n",
    "ax2_2 = ax2.twinx();\n",
    "ax2_2.plot(daily.index, daily['count'], 'r--', alpha=0.6, linewidth=2);\n",
    "ax2_2.grid(False);\n",
    "ax2_2.set_ylabel('Article Count');\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"{}_scores\".format('_'.join(name.split()).lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make calendar plot to show periods of activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = calmap.calendarplot(data['mean_score'],\n",
    "                    vmin = -1.0,\n",
    "                    vmax=1.0,\n",
    "                    daylabels='MTWTFSS',\n",
    "                    dayticks=[0, 2, 4, 6],\n",
    "                    fig_kws=dict(figsize=(12.5, 9)),\n",
    "                    linewidth=1,\n",
    "                    fillcolor='lightgrey',\n",
    "                    cmap='coolwarm_r',\n",
    "                   );\n",
    "fig.suptitle(\"Calendar map of aggregated sentiment for {}\".format(name), fontsize=18);\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "if write_:\n",
    "    out_filename = '_'.join(name.split()).lower()\n",
    "    plt.savefig('calmap_{}.png'.format(out_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get counts of positive and negative mentions based on Publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grouped = news_relevant.groupby('publication').apply(lambda x: x['score'] >= 0.0)\n",
    "grouped = grouped.groupby('publication').value_counts().to_frame()\n",
    "grouped = grouped.unstack().fillna(0.0)\n",
    "grouped.columns = ['Negative', 'Positive']\n",
    "grouped = grouped.sort_values(by='Negative')\n",
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot article breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped.plot(kind='barh', figsize=(12, 8));\n",
    "plt.title('Count of number of articles with Positive/Negative Sentiment for {}'.format(name));\n",
    "# plt.savefig(\"{}_breakdown\".format('_'.join(name.split()).lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output result to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_:\n",
    "    out_filename = '_'.join(name.split()).lower() + '_breakdown.csv'\n",
    "    out_path = Path('./') / \"results/flair\" / out_filename\n",
    "    grouped.to_csv(out_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_: \n",
    "    data_filename = '_'.join(name.split()).lower() + '_data.csv'\n",
    "    data_path = Path('./') / \"results/flair\" / data_filename\n",
    "    daily[~daily['relevant'].eq(0)].to_csv(data_path, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Cosine Similarity Distances\n",
    "To see how similar or different each article is based on publication, we can compute the cosine distances between articles to generate a \"distance matrix\" and then visualize these distances in two-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate TF-IDF for document similarity\n",
    "We first define the term frequency-inverse document frequency to vectorize the text for each article into parameters, and generate a ```tf-idf``` matrix. \n",
    "\n",
    "Once we compute the ```tf-idf``` matrix, we can find a \"distance matrix\" that stores how similar or how different two documents are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2)\n",
    "# \n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(news_relevant['lemmas'] ) #fit the vectorizer to synopses\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "# Display some key terms\n",
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cosine distance matrix\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=37)\n",
    "dist_transformed = embedding.fit_transform(dist)\n",
    "print(dist_transformed.shape)\n",
    "\n",
    "xs, ys = dist_transformed[:, 0], dist_transformed[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate an MDS DataFrame for plotting\n",
    "We combine the x-y distances from the MDS calculation with the original publication labels to see how different the articles are from each other, colored by publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = pd.DataFrame(dict(label=news_relevant['publication'], x=xs, y=ys))\n",
    "compare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = news_relevant['publication'].nunique()\n",
    "print(\"Found {} unique categories for publications\".format(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = compare.groupby('label').agg({'label': 'count', 'x': 'mean', 'y': 'mean'})\n",
    "groups.columns = ['count', 'x', 'y']\n",
    "groups = groups.sort_values(by='count')\n",
    "groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize similarities as embedded cosine distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "colors = [i for i in range(len(groups.index))]\n",
    "\n",
    "ax.scatter(groups['x'], groups['y'], c=colors, \n",
    "            s=groups['count']*100, linewidths=1.5, alpha=0.7,\n",
    "            edgecolors='k', cmap=plt.cm.gist_rainbow,\n",
    "            );\n",
    "\n",
    "for i, txt in enumerate(groups.index):\n",
    "    ax.annotate(txt, (groups['x'][i], groups['y'][i]),\n",
    "                fontsize=18, alpha=0.7);\n",
    "ax.set_xticklabels(['']);\n",
    "ax.set_yticklabels(['']);\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
